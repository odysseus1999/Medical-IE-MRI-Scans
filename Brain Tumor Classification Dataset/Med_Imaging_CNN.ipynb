{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWzxYPaDCNjG"
      },
      "source": [
        "## Import Libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bfuiKmeeFPe-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "import skimage.measure\n",
        "from skimage import data\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from skimage.morphology import disk\n",
        "from skimage.util import img_as_ubyte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "752EWVNIB4PY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaqXEpA6CmH2"
      },
      "source": [
        "## DATASET\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsyU4pp-EWPp",
        "outputId": "6c302014-dfff-45bd-dd2e-847b9263b2d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kHpjl4A8EzJN"
      },
      "outputs": [],
      "source": [
        "main_dir = \"/content/drive/MyDrive/TCC - Parte II/\"\n",
        "dataset_path = main_dir + \"Brain Tumor Classification Dataset\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uLyOAOPQ-9F"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg8IESkWFLXm"
      },
      "source": [
        "###Getting Info from dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GOOCqgygFWZ0"
      },
      "outputs": [],
      "source": [
        "train_glioma = glob.glob(dataset_path + \"/Training/glioma_tumor/*.jpg\")\n",
        "train_menignioma = glob.glob(dataset_path + '/Training/meningioma_tumor/*.jpg')\n",
        "train_pituitary = glob.glob( dataset_path + '/Training/pituitary_tumor/*.jpg')\n",
        "train_no = glob.glob(dataset_path + '/Training/no_tumor/*.jpg')\n",
        "\n",
        "#Test\n",
        "test_glioma = glob.glob( dataset_path + '/Testing/glioma_tumor/*.jpg')\n",
        "test_menignioma = glob.glob(dataset_path +  '/Testing/meningioma_tumor/*.jpg')\n",
        "test_pituitary = glob.glob(dataset_path + '/Testing/pituitary_tumor/*.jpg')\n",
        "test_no = glob.glob(dataset_path + '/Testing/no_tumor/*.jpg')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDLU1W6WFWUW",
        "outputId": "ca8ff5ad-f352-4361-ebe0-f507c7c37cbb"
      },
      "outputs": [],
      "source": [
        "train_glioma_number = len(train_glioma)\n",
        "train_menignioma_number = len(train_menignioma)\n",
        "train_pituitary_number = len(train_pituitary)\n",
        "train_no_number = len(train_no)\n",
        "\n",
        "test_glioma_number = len(test_glioma)\n",
        "test_menignioma_number = len(test_menignioma)\n",
        "test_pituitary_number = len(test_pituitary)\n",
        "test_no_number = len(test_no)\n",
        "\n",
        "print(\"Number of train_glioma: \",train_glioma_number)\n",
        "print(\"Number of train_menignioma: \",train_menignioma_number)\n",
        "print(\"Number of train_pituitary: \",train_pituitary_number)\n",
        "print(\"Number of train_no: \",train_no_number)\n",
        "print(\"Number of test_glioma: \",test_glioma_number)\n",
        "print(\"Number of test_menignioma: \",train_menignioma_number)\n",
        "print(\"Number of test_pituitary: \",train_pituitary_number)\n",
        "print(\"Number of test_no: \",train_no_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "2t-KfEe-FKwI",
        "outputId": "336d6e70-2426-4eaa-b16b-7dcf2680185f"
      },
      "outputs": [],
      "source": [
        "full_glioma = train_glioma_number + test_glioma_number\n",
        "full_no_tumor = train_no_number + test_no_number\n",
        "full_menignioma = train_menignioma_number + test_menignioma_number\n",
        "full_pituitary = train_pituitary_number + test_pituitary_number\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "colors = ['#8CAF99','#E198AB',\"#89BEDB\",\"#C6A7D2\"]\n",
        "labels = ['Glioma', 'Meningioma', 'Glândula Pituitária', 'Sem tumor']\n",
        "\n",
        "plt.rcParams.update({'font.size': 8})\n",
        "plt.pie([full_glioma,\n",
        "         full_menignioma,full_pituitary,full_no_tumor],\n",
        "        labels=labels,\n",
        "        colors=colors, autopct='%.1f%%', explode=(0.025,0.025,0.025,0.025),\n",
        "        startangle=30);\n",
        "# plt.savefig(\"Distribuição do dataset.png\", dpi=300)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9oh6d8IrXcV"
      },
      "source": [
        "##Pre Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5P1Gs1ZOrZmZ"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "def resize_images(dataset_path, output_path, target_size=(256, 256)):\n",
        "    \"\"\"\n",
        "    Redimensiona todas as imagens em um dataset para o mesmo tamanho.\n",
        "\n",
        "    Parâmetros:\n",
        "    - dataset_path: Caminho para o diretório que contém as imagens do dataset.\n",
        "    - output_path: Caminho para o diretório de saída onde as imagens redimensionadas serão salvas.\n",
        "    - target_size: Tupla representando o tamanho desejado das imagens (largura, altura).\n",
        "\n",
        "    Retorna:\n",
        "    - None\n",
        "    \"\"\"\n",
        "\n",
        "    # Certifique-se de que o diretório de saída existe\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    # Lista todos os arquivos no diretório do dataset\n",
        "    image_files = [f for f in os.listdir(dataset_path) if os.path.isfile(os.path.join(dataset_path, f))]\n",
        "\n",
        "    for image_file in image_files:\n",
        "      add_pixels = 0\n",
        "      # Caminho completo da imagem original\n",
        "      input_image_path = os.path.join(dataset_path, image_file)\n",
        "      # Carregue a imagem\n",
        "      img = cv2.imread(input_image_path)\n",
        "      img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "\n",
        "\n",
        "      thresh = cv2.threshold(img_gray, 45, 255, cv2.THRESH_BINARY)[1]\n",
        "      # find contours in thresholded image, then grab the largest one\n",
        "      cnts, _ = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "      c = max(cnts, key=cv2.contourArea)\n",
        "      # find the extreme points\n",
        "      extLeft = tuple(c[c[:, :, 0].argmin()][0])\n",
        "      extRight = tuple(c[c[:, :, 0].argmax()][0])\n",
        "      extTop = tuple(c[c[:, :, 1].argmin()][0])\n",
        "      extBot = tuple(c[c[:, :, 1].argmax()][0])\n",
        "\n",
        "      # create a new image by cropping the region defined by extreme points\n",
        "      new_img = img[extTop[1] - add_pixels:extBot[1] + add_pixels, extLeft[0] - add_pixels:extRight[0] + add_pixels].copy()\n",
        "\n",
        "\n",
        "      #Carrega a imagem\n",
        "      resized_img = cv2.resize(new_img, target_size)\n",
        "\n",
        "\n",
        "      # Redimensione a imagem\n",
        "\n",
        "\n",
        "      # Crie o caminho de saída e salve a imagem redimensionada\n",
        "      output_image_path = os.path.join(output_path, image_file)\n",
        "      cv2.imwrite(output_image_path, resized_img)\n",
        "\n",
        "\n",
        "# Exemplo de uso\n",
        "main_dir = \"/content/drive/MyDrive/TCC - Parte II/\"\n",
        "dataset_path = main_dir + \"Brain Tumor Classification Dataset\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3ixr-ukHzHOL"
      },
      "outputs": [],
      "source": [
        "\n",
        "##Testing\n",
        "\n",
        "glioma_dataset = dataset_path + \"/Testing/glioma_tumor\"\n",
        "glioma_output = dataset_path + \"/preprocessing\" + \"/Testing/glioma_tumor\"\n",
        "\n",
        "resize_images(glioma_dataset, glioma_output, target_size=(256, 256))\n",
        "\n",
        "meningioma_dataset = dataset_path + \"/Testing/meningioma_tumor\"\n",
        "meningioma_output = dataset_path + \"/preprocessing\" + \"/Testing/meningioma_tumor\"\n",
        "\n",
        "resize_images(meningioma_dataset, meningioma_output, target_size=(256, 256))\n",
        "\n",
        "no_tumor_dataset = dataset_path + \"/Testing/no_tumor\"\n",
        "no_tumor_output = dataset_path + \"/preprocessing\" + \"/Testing/no_tumor\"\n",
        "\n",
        "resize_images(no_tumor_dataset, no_tumor_output, target_size=(256, 256))\n",
        "\n",
        "pituitary_dataset = dataset_path + \"/Testing/pituitary_tumor\"\n",
        "pituitary_output = dataset_path + \"/preprocessing\" + \"/Testing/pituitary_tumor\"\n",
        "\n",
        "resize_images(pituitary_dataset, pituitary_output, target_size=(256, 256))\n",
        "\n",
        "#############\n",
        "\n",
        "glioma_dataset = dataset_path + \"/Training/glioma_tumor\"\n",
        "glioma_output = dataset_path + \"/preprocessing\" + \"/Training/glioma_tumor\"\n",
        "\n",
        "resize_images(glioma_dataset, glioma_output, target_size=(256, 256))\n",
        "\n",
        "meningioma_dataset = dataset_path + \"/Training/meningioma_tumor\"\n",
        "meningioma_output = dataset_path + \"/preprocessing\" + \"/Training/meningioma_tumor\"\n",
        "\n",
        "resize_images(meningioma_dataset, meningioma_output, target_size=(256, 256))\n",
        "\n",
        "no_tumor_dataset = dataset_path + \"/Training/no_tumor\"\n",
        "no_tumor_output = dataset_path + \"/preprocessing\" + \"/Training/no_tumor\"\n",
        "\n",
        "resize_images(no_tumor_dataset, no_tumor_output, target_size=(256, 256))\n",
        "\n",
        "pituitary_dataset = dataset_path + \"/Training/pituitary_tumor\"\n",
        "pituitary_output = dataset_path + \"/preprocessing\" + \"/Training/pituitary_tumor\"\n",
        "\n",
        "resize_images(pituitary_dataset, pituitary_output, target_size=(256, 256))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugy_R9E8CTUO"
      },
      "source": [
        "## Building the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GscMoif9h6N6"
      },
      "outputs": [],
      "source": [
        "def validate_model(model, val_loader, criterion, device='cuda'):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    all_val_outputs = []\n",
        "    all_val_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for val_inputs, val_targets in val_loader:\n",
        "            # Mova os dados para o dispositivo\n",
        "            val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
        "\n",
        "            # Obtenha as previsões do modelo\n",
        "            val_outputs = model(val_inputs)\n",
        "\n",
        "            # Calcule a perda\n",
        "            loss = criterion(val_outputs, val_targets)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            all_val_outputs.append(val_outputs)\n",
        "            all_val_targets.append(val_targets)\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    val_outputs = torch.cat(all_val_outputs, dim=0)\n",
        "    val_targets = torch.cat(all_val_targets, dim=0)\n",
        "\n",
        "    # Mova as previsões e alvos para a CPU para calcular a acurácia\n",
        "    val_outputs_cpu = val_outputs.cpu()\n",
        "    val_predictions = torch.argmax(val_outputs_cpu, axis=1)\n",
        "    val_targets_cpu = val_targets.cpu()\n",
        "\n",
        "    # Calcula as previsões e acurácia\n",
        "    val_accuracy = accuracy_score(val_targets_cpu.numpy(), val_predictions.numpy())\n",
        "\n",
        "    return val_loss, val_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "pn_WO65yCYXg",
        "outputId": "89aa5f38-f41e-4d29-8196-160921f5a00e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Verifique se o ambiente do Google Colab possui GPU habilitada\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Transformações de pré-processamento das imagens\n",
        "# Verificar Melhor isso\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Carregar o conjunto de dados\n",
        "train_dataset = datasets.ImageFolder(root= dataset_path + '/Training', transform = transform)\n",
        "val_dataset = datasets.ImageFolder(root = dataset_path + '/Testing', transform = transform)\n",
        "\n",
        "# Definir DataLoader para carregar os dados em lotes\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=1)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Definir arquitetura da rede neural\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(128 * 56 * 56, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 128 * 56 * 56)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "# Instanciar o modelo\n",
        "model = SimpleCNN(num_classes=4).to(device)\n",
        "\n",
        "# Definir a função de perda e otimizador\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Treinar o modelo\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validação\n",
        "    val_loss, val_accuracy = validate_model(model, val_loader, criterion)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Val Loss: {val_loss:.2f}, Val Accuracy: {val_accuracy:.2f}')\n",
        "\n",
        "\n",
        "torch.save({\n",
        "    'epoch': num_epochs,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss': loss,\n",
        "}, main_dir + 'Models/modelo_treinado.pth')\n",
        "\n",
        "print(\"Treinamento concluído.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPXupwxcMHf_"
      },
      "outputs": [],
      "source": [
        "# Obtém informações sobre o dispositivo\n",
        "print(\"Dispositivo atual:\", device)\n",
        "print(\"Nome do dispositivo:\", torch.cuda.get_device_name(device) if device.type == \"cuda\" else \"CPU\")\n",
        "print(\"Capacidade de computação da GPU:\", torch.cuda.get_device_capability(device) if device.type == \"cuda\" else \"N/A\")\n",
        "print(\"Número de GPUs disponíveis:\", torch.cuda.device_count() if torch.cuda.is_available() else 0)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
